---
title: "Homework 2"
author: "Nianyu Li"
date: "`r Sys.Date()`"
output:
  html_document:
    theme: flatly
    highlight: zenburn
    number_sections: yes
    toc: yes
    toc_float: yes
    code_folding: show
  pdf_document:
    toc: yes
---

------------------------------------------------------------------------

```{r}
#| label: load-libraries
#| echo: false # This option disables the printing of code (only output is displayed).
#| message: false
#| warning: false

library(tidyverse)
library(wbstats)
library(skimr)
library(countrycode)
library(here)
library(lubridate)
```

# Data Visualisation - Exploration

Now that you've demonstrated your software is setup, and you have the basics of data manipulation, the goal of this assignment is to practice transforming, visualising, and exploring data.

# Mass shootings in the US

In July 2012, in the aftermath of a mass shooting in a movie theater in Aurora, Colorado, [Mother Jones](https://www.motherjones.com/politics/2012/07/mass-shootings-map/) published a report on mass shootings in the United States since 1982. Importantly, they provided the underlying data set as [an open-source database](https://www.motherjones.com/politics/2012/12/mass-shootings-mother-jones-full-data/) for anyone interested in studying and understanding this criminal behavior.

## Obtain the data

```{r}
#| echo: false
#| message: false
#| warning: false


mass_shootings <- read_csv(here::here("data", "mass_shootings.csv"))

glimpse(mass_shootings)
```

| column(variable)     | description                                                                 |
|----------------------|-----------------------------------------------------------------------------|
| case                 | short name of incident                                                      |
| year, month, day     | year, month, day in which the shooting occurred                             |
| location             | city and state where the shooting occcurred                                 |
| summary              | brief description of the incident                                           |
| fatalities           | Number of fatalities in the incident, excluding the shooter                 |
| injured              | Number of injured, non-fatal victims in the incident, excluding the shooter |
| total_victims        | number of total victims in the incident, excluding the shooter              |
| location_type        | generic location in which the shooting took place                           |
| male                 | logical value, indicating whether the shooter was male                      |
| age_of_shooter       | age of the shooter when the incident occured                                |
| race                 | race of the shooter                                                         |
| prior_mental_illness | did the shooter show evidence of mental illness prior to the incident?      |

## Explore the data

### Specific questions

-   Generate a data frame that summarizes the number of mass shootings per year.

```{r}
mass_shootings %>% 
  group_by(year) %>% 
  summarize(count = n())
```

-   Generate a bar chart that identifies the number of mass shooters associated with each race category. The bars should be sorted from highest to lowest and each bar should show its number.

```{r}
#Create a table that shows the number of shooters in each race category
shooters_race <- mass_shootings %>%
  #Remove the NA in race
  filter(!is.na(race)) %>% 
  group_by(race) %>% 
  summarize(count = n())

print(shooters_race)

#Plot the table above.
shooters_race %>% 
  
  #sort the bars from highest to lowest
  mutate(race = fct_reorder(race, count)) %>% 
  
  ggplot()+
  
  aes(x = count,
      y = race) +
  
  geom_col(fill = "#001E62") +
  
  #add labels to show # of shooters on each bar
  geom_text(aes(label = count),
                hjust = 1,
                color = "#FFFFFF",
                size = 5) +
  
  #add titles
  labs(title = "Which race is associated with most shooters",
       subtitle = "Number of shooters in each race",
       x = "Number of shooter",
       y = NULL) +
  
  theme_void() +
  #adjust the positions and sizes
  theme(plot.title.position = "plot",
        axis.text = element_text(size=12),
        plot.title = element_text(size=18))+
  
  NULL
  
```

-   Generate a boxplot visualizing the number of total victims, by type of location.

```{r warning=FALSE}
#Create a table that shows the number of total victims in each case by type of location
victims_location_type <- 
  mass_shootings %>%
  select(location_type,
         total_victims)

print(victims_location_type)

#Note that 95% of the data is below 44:
quantile(mass_shootings$total_victims, probs = .95)

#Plot the table above in a box plot
victims_location_type %>% 
  
  ggplot()+
  
  aes(x = location_type,
      y = total_victims)+
  
  #Create the box plot and remove outliers
  geom_boxplot()+
  scale_y_continuous(limits = c(0,50))+
  
  #add titles
  labs(title = "Which type of location had the most victims",
       subtitle = "total victims in each case by type of location",
       x = "Location Type",
       y = "Total Victims",
       caption = "Note: 4 extreme values removed:
       Other: 604, 102, 82; School: 55") +
  
  theme_light()+
                  
  NULL
  
```

-   Redraw the same plot, but remove the Las Vegas Strip massacre from the dataset.

It can be done using the method in the question above, i.e.setting a limit on y axis. We can also filter out the outlier in the first place.

```{r}
victims_location_type_1 <- 
  mass_shootings %>%
  select(location_type,
         total_victims) %>% 
  filter(total_victims!=604)

print(victims_location_type_1)

#Plot the table above in a box plot
victims_location_type_1 %>% 
  
  ggplot()+
  
  aes(x = location_type,
      y = total_victims)+
  
  geom_boxplot()+
  
  #add titles
  labs(title = "Which type of location had the most victims",
       subtitle = "total victims in each case by type of location",
       x = "Location Type",
       y = "Total Victims",
       caption = "Note: 1 extreme values removed:
       Other: 604 in Las Vegas Strip massacre") +
  
  theme_light()+
                  
  NULL
```

### More open-ended questions

Address the following questions. Generate appropriate figures/tables to support your conclusions.

-   How many white males with prior signs of mental illness initiated a mass shooting after 2000?

```{r}
#Filter out the data using the criteria above
mass_shootings %>% 
  filter(race == "White",
         male == TRUE,
         prior_mental_illness == "Yes",
         year > 2000) %>% 
  count()

#Filtered data shows that there are 22 of them.
```

-   Which month of the year has the most mass shootings? Generate a bar chart sorted in chronological (natural) order (Jan-Feb-Mar- etc) to provide evidence of your answer.

Feburary has the most mass shootings. See bar chart below.

```{r}
#Create a table to show mass shootings in each month of the year
shootings_by_month <- 
  mass_shootings %>%
  
  #convert month to a factor
  mutate(month = factor(month, levels = month.abb)) %>% 
  
  #group by month and count # of shootings in each month
  group_by(month) %>% 
  summarise(number = n())

print(shootings_by_month)

#Plot the table

shootings_by_month %>% 
  
  ggplot()+
  
  aes(x = month,
      y = number)+
  
  geom_bar(stat = "identity",
           fill = "#001E62")+
  
  #add titles
  labs(title = "Which month did most shootings happened",
       subtitle = "Number of shootings in each month",
       x = "Month",
       y = "Number of shootings")+
  
  theme_classic()+
  
  NULL
```

-   How does the distribution of mass shooting fatalities differ between White and Black shooters? What about White and Latino shooters?

Black and white shooter: in most cases, fatalities caused by black shooters are smaller than that of white shooters. As the density plot indicates, fatalities by black shooters are concentrated on the left hand side of fatalities by white shooters. In the white shooters case, the density plot has a longer tail, which means fatalities are more disperse.

White and Latino shooters: in most cases, fatalities caused by latino shooters are also smaller than that of white shooters. They are also more concentrated on the left side of the plot, compared to that caused by white shooters.

See density plot below.

```{r}
#Create a table to show shootings fatalities by shooters race
fatalities_race <- 
  mass_shootings %>%
  #filter out data that is not NA in race
  filter(!is.na(race)) %>%
  select(fatalities, race)

print(fatalities_race)

#Plot the distribution of fatalities, using geom_density
fatalities_race %>%  
  
  ggplot()+
  
  aes(x = fatalities,
      fill = race)+
  
  geom_density(color = NA,alpha = 0.5)+
  
  scale_x_continuous(breaks = seq(0,60,5))+
  
  theme_classic()+
  
  #add titles
  labs(title = "How do fatalities differ among shooters from different races",
       subtitle = "Distribution of fatalities associated with shooter in each race",
       x = "Fatalities",
       y = NULL)+
  
  NULL
```

### Very open-ended

-   Are mass shootings with shooters suffering from mental illness different from mass shootings with no signs of mental illness in the shooter?

Shootings were more likely to be conducted by shooters with mental illness. Location types are also different. Shooters with mental illness have a wider choices of locations and they can shoot in public spaces like airports and religious places.

See pie chart below for percentages of type of locations for shooters with and without mental illness.

```{r}
#Create a table to show percent of type of locations for the two type of shooters.
shooters_mental_location <- 
  mass_shootings %>% 
  #filter out NAs
  filter(!is.na(prior_mental_illness)) %>% 
  group_by(prior_mental_illness,
           location_type) %>% 
  summarize(count = n())

print(shooters_mental_location)

#Create a pie chart to show differences in location type by the two type of shooters

shooters_mental_location %>% 
  
  ggplot()+
  
  aes(x="",
      y = count,
      fill = location_type)+
  
  geom_bar(stat="identity", width=1)+
  coord_polar("y", start=0)+
  
  facet_wrap(~prior_mental_illness)+
  
  scale_fill_brewer(palette="Set2")+
  
  theme_minimal()+
  
  #add titles
  labs(title = "How do shooters with and without mental illness differ in choices of locations",
       subtitle = "Location type associated with two type of shooters",
       x = NULL)+
  
  NULL
  
```

-   Assess the relationship between mental illness and total victims, mental illness and location type, and the intersection of all three variables.

Mental illness and total victims: shooters with mental illness were causing more victims than those without.

Mental illness and location type: shooters with mental illness chose locations more randomly, while those without mainly focused on others, schools and workplaces.

Intersection: shooters with mental illness shooting at school and workplaces were causing most of the victims.

See bar plots below.

```{r}
#Create a table that shows all three variables above
mental_victims_location <- 
  mass_shootings %>% 
  #filter out NAs
  filter(!is.na(prior_mental_illness)) %>% 
  select(prior_mental_illness,
         total_victims,
         location_type)

print(mental_victims_location)

#Bar plots
mental_victims_location %>% 
  
  ggplot()+
  
  aes(x = location_type,
      y = total_victims,
      fill = prior_mental_illness)+
  
  geom_bar(stat = "identity")+
  
  scale_fill_brewer(palette="Blues",
                    name  ="Prior Mental Illness")+
  
  theme_minimal()+
  
  #add titles
  labs(title = "How do shooters with and without mental illness differ in number of victims",
       subtitle = "number of victims associated with two type of shooters",
       x = "Type of Location",
       y = "Number of Victims")+
  
  NULL

```

Make sure to provide a couple of sentences of written interpretation of your tables/figures. Graphs and tables alone will not be sufficient to answer this question.

# Exploring credit card fraud

We will be using a dataset with credit card transactions containing legitimate and fraud transactions. Fraud is typically well below 1% of all transactions, so a naive model that predicts that all transactions are legitimate and not fraudulent would have an accuracy of well over 99%-- pretty good, no? (well, not quite as we will see later in the course)

You can read more on credit card fraud on [Credit Card Fraud Detection Using Weighted Support Vector Machine](https://www.scirp.org/journal/paperinformation.aspx?paperid=105944)

The dataset we will use consists of credit card transactions and it includes information about each transaction including customer details, the merchant and category of purchase, and whether or not the transaction was a fraud.

## Obtain the data

The dataset is too large to be hosted on Canvas or Github, so please download it from dropbox <https://www.dropbox.com/sh/q1yk8mmnbbrzavl/AAAxzRtIhag9Nc_hODafGV2ka?dl=0> and save it in your `dsb` repo, under the `data` folder

```{r}
#| echo: false
#| message: false
#| warning: false

card_fraud <- read_csv(here::here("data", "card_fraud.csv"))

glimpse(card_fraud)
```

The data dictionary is as follows

| column(variable)      | description                                 |
|-----------------------|---------------------------------------------|
| trans_date_trans_time | Transaction DateTime                        |
| trans_year            | Transaction year                            |
| category              | category of merchant                        |
| amt                   | amount of transaction                       |
| city                  | City of card holder                         |
| state                 | State of card holder                        |
| lat                   | Latitude location of purchase               |
| long                  | Longitude location of purchase              |
| city_pop              | card holder's city population               |
| job                   | job of card holder                          |
| dob                   | date of birth of card holder                |
| merch_lat             | Latitude Location of Merchant               |
| merch_long            | Longitude Location of Merchant              |
| is_fraud              | Whether Transaction is Fraud (1) or Not (0) |

-   In this dataset, how likely are fraudulent transactions? Generate a table that summarizes the number and frequency of fraudulent transactions per year.

In 2019, there were 2721 fraudulent transactions, occurring at a frequency of 0.57%. In 2020, there were 1215 fraudulent transactions, occurring at a frequency of 0.63%.

```{r}
fraud_per_year <- 
  card_fraud %>% 
  group_by(trans_year) %>% 
  summarise(number = sum(is_fraud),
            frequency_in_percent = round(number/n()*100,2)) %>% 
  ungroup()

print(fraud_per_year)
```

-   How much money (in US\$ terms) are fraudulent transactions costing the company? Generate a table that summarizes the total amount of legitimate and fraudulent transactions per year and calculate the % of fraudulent transactions, in US\$ terms.

See table for totl amount of legitimate and fraudulent transactions per year. Calculate % of fraudulent transactions percent_fraud_amount_2019:`r round(1423140.0/(32182901.2+1423140.0)*100,2)`% percent_fraud_amount_2020:`r round(651949.2/(12925914.0+651949.2)*100,2)`%

```{r}
#Create the table to show the amount of legitimate and fraudulent transactions per year
amount_fraud_per_year <- 
  card_fraud %>% 
  group_by(trans_year, is_fraud) %>% 
  summarize(amount = sum(amt)) %>% 
  ungroup()

print(amount_fraud_per_year)
```

-   Generate a histogram that shows the distribution of amounts charged to credit card, both for legitimate and fraudulent accounts. Also, for both types of transactions, calculate some quick summary statistics.

```{r}
#Create a table to show amounts in each transaction
amount_charged_legit <- 
  card_fraud %>%
  filter(is_fraud == 0) %>% 
  select(amt)

amount_charged_fraud <- 
  card_fraud %>%
  filter(is_fraud == 1) %>% 
  select(amt)

#Plot the distribution for legit transactions
amount_charged_legit %>% 
  
  ggplot()+
  
  aes(x = amt)+
  
  geom_histogram(fill = "#001E62")+
  
  theme_classic()+
  
  #add titles
  labs(title = "Distribution of Amount in Legitimate Transactions",
       x = "Amount",
       y = NULL)+
  
  NULL

#Plot the distribution for fraud transactions
amount_charged_fraud %>% 
  
  ggplot()+
  
  aes(x = amt)+
  
  geom_histogram(fill = "#001E62")+
  
  theme_classic()+
  
  #add titles
  labs(title = "Distribution of Amount in Fraudulent Transactions",
       x = "Amount",
       y = NULL)+
  
  NULL

#Summary of statistics
summary(amount_charged_legit)
summary(amount_charged_fraud)
```

-   What types of purchases are most likely to be instances of fraud? Consider category of merchants and produce a bar chart that shows % of total fraudulent transactions sorted in order.

```{r}
#Create a table to show # of transactions by merchants.
merchant_type <- 
  card_fraud %>%
  group_by(category, is_fraud) %>%
  
  #Calculate the percent of fraudulent transactions in each category
  summarise(count = n()) %>% 
  mutate(pct = round(count/sum(count)*100, 2)) %>%
  
  #Remove unnecessary data
  filter(is_fraud == 1) %>% 
  select(category, pct) %>%
  
  #Sort in descending order by pct
  arrange(desc(pct)) %>% 
  print()

#Bar chart
merchant_type %>% 
  
  ggplot()+
  
  aes(x= reorder(category,-pct),
      y = pct)+
  
  geom_bar(stat = "identity",
           fill = "#001E62")+
  
  theme_minimal()+
  theme(axis.text.x = element_text(angle = 45, vjust = 1, hjust=1))+
  
  #add titles
  labs(title = "Which type of merchants are more likely to be instances of fraud",
       subtitle = "% of total fraudulent transactions in each type of merchant",
       x = "Type of Merchant",
       y = "% of fraudulent transactions")+
  
  NULL
```

-   When is fraud more prevalent? Which days, months, hours? To create new variables to help you in your analysis, we use the `lubridate` package and the following code

We can tell from the table below that fraud is most prevalent: 1) at 23:00 2) on 2019-02-25 3) in March 4) on Mondays

<!-- -->

    mutate(
      date_only = lubridate::date(trans_date_trans_time),
      month_name = lubridate::month(trans_date_trans_time, label=TRUE),
      hour = lubridate::hour(trans_date_trans_time),
      weekday = lubridate::wday(trans_date_trans_time, label = TRUE)
      )
      

```{r}
#Create new variables
time_fraud <- 
  card_fraud %>% 
  mutate(
      date_only = lubridate::date(trans_date_trans_time),
      month_name = lubridate::month(trans_date_trans_time, label=TRUE),
      hour = lubridate::hour(trans_date_trans_time),
      weekday = lubridate::wday(trans_date_trans_time, label = TRUE)
      )

#See which hours are frauds most prevalent
time_fraud %>% 
  group_by(hour) %>% 
  summarise(count = sum(is_fraud)) %>% 
  arrange(desc(count)) %>% 
  print()

#See which days are frauds most prevalent
time_fraud %>% 
  group_by(date_only) %>% 
  summarise(count = sum(is_fraud)) %>% 
  arrange(desc(count)) %>% 
  print()

#See which month are frauds most prevalent
time_fraud %>% 
  group_by(month_name) %>% 
  summarise(count = sum(is_fraud)) %>% 
  arrange(desc(count)) %>% 
  print()

#See which weekdays are frauds most prevalent
time_fraud %>% 
  group_by(weekday) %>% 
  summarise(count = sum(is_fraud)) %>% 
  arrange(desc(count)) %>% 
  print()

```

-   Are older customers significantly more likely to be victims of credit card fraud? To calculate a customer's age, we use the `lubridate` package and the following code

From the density plot, we can tell that it is tilted towards the left hand side, which means that young people are actually more likely to be victims of frauds.

<!-- -->

      mutate(
       age = interval(dob, trans_date_trans_time) / years(1),
        )

```{r}
age_fraud <- 
  card_fraud %>%
  #Create new variable that shows customer age
  mutate(
       age = interval(dob, trans_date_trans_time) / years(1),
        ) %>% 
  filter(is_fraud==1)
  
#See number of fraud in different ages
age_fraud %>% 
  
  ggplot()+
  
  aes(x = age)+
  
  geom_density(color = NA, fill = "#001E62", alpha = 0.5)+
  
  theme_classic()+
  
  #add titles
  labs(title = "Are frauds more likely to happen with elder people",
       subtitle = "Distribution of age in frauds",
       x = "Age",
       y = NULL)+
  
  NULL
```

-   Is fraud related to distance? The distance between a card holder's home and the location of the transaction can be a feature that is related to fraud. To calculate distance, we need the latidue/longitude of card holders's home and the latitude/longitude of the transaction, and we will use the [Haversine formula](https://en.wikipedia.org/wiki/Haversine_formula) to calculate distance. I adapted code to [calculate distance between two points on earth](https://www.geeksforgeeks.org/program-distance-two-points-earth/amp/) which you can find below

```{r}
# distance between card holder's home and transaction
# code adapted from https://www.geeksforgeeks.org/program-distance-two-points-earth/amp/


card_fraud <- card_fraud %>%
  mutate(
    
    # convert latitude/longitude to radians
    lat1_radians = lat / 57.29577951,
    lat2_radians = merch_lat / 57.29577951,
    long1_radians = long / 57.29577951,
    long2_radians = merch_long / 57.29577951,
    
    # calculate distance in miles
    distance_miles = 3963.0 * acos((sin(lat1_radians) * sin(lat2_radians)) + cos(lat1_radians) * cos(lat2_radians) * cos(long2_radians - long1_radians)),

    # calculate distance in km
    distance_km = 6377.830272 * acos((sin(lat1_radians) * sin(lat2_radians)) + cos(lat1_radians) * cos(lat2_radians) * cos(long2_radians - long1_radians))

  )

```

Plot a boxplot or a violin plot that looks at the relationship of distance and `is_fraud`. Does distance seem to be a useful feature in explaining fraud?

It turned out that two box plots looks more or less the same. Therefore, distance seems not a good explanatory feature for fraud.

```{r}
#Create boxplot
card_fraud %>% 
  
  ggplot()+
  aes(x = distance_km)+
  
  geom_boxplot()+
  
  facet_wrap(~is_fraud)+
  
  theme_classic()+
  
  #add titles
  labs(title = "Are fraud correlated with distance between merchants and buyers",
       subtitle = "Relationship between distance and fraud",
       x = "Distance",
       y = "Possibility of fraud")+
  
  NULL

```

# Exploring sources of electricity production, CO2 emissions, and GDP per capita.

There are many sources of data on how countries generate their electricity and their CO2 emissions. I would like you to create three graphs:

## 1. A stacked area chart that shows how your own country generated its electricity since 2000.

You will use

`geom_area(colour="grey90", alpha = 0.5, position = "fill")`

## 2. A scatter plot that looks at how CO2 per capita and GDP per capita are related

## 3. A scatter plot that looks at how electricity usage (kWh) per capita/day GDP per capita are related

We will get energy data from the Our World in Data website, and CO2 and GDP per capita emissions from the World Bank, using the `wbstats`package.

```{r}
#| message: false
#| warning: false

# Download electricity data
url <- "https://nyc3.digitaloceanspaces.com/owid-public/data/energy/owid-energy-data.csv"

energy <- read_csv(url) %>% 
  filter(year >= 1990) %>% 
  drop_na(iso_code) %>% 
  select(1:3,
         biofuel = biofuel_electricity,
         coal = coal_electricity,
         gas = gas_electricity,
         hydro = hydro_electricity,
         nuclear = nuclear_electricity,
         oil = oil_electricity,
         other_renewable = other_renewable_exc_biofuel_electricity,
         solar = solar_electricity,
         wind = wind_electricity, 
         electricity_demand,
         electricity_generation,
         net_elec_imports,	# Net electricity imports, measured in terawatt-hours
         energy_per_capita,	# Primary energy consumption per capita, measured in kilowatt-hours	Calculated by Our World in Data based on BP Statistical Review of World Energy and EIA International Energy Data
         energy_per_gdp,	# Energy consumption per unit of GDP. This is measured in kilowatt-hours per 2011 international-$.
         per_capita_electricity, #	Electricity generation per capita, measured in kilowatt-hours
  ) 

# Download data for C02 emissions per capita https://data.worldbank.org/indicator/EN.ATM.CO2E.PC
co2_percap <- wb_data(country = "countries_only", 
                      indicator = "EN.ATM.CO2E.PC", 
                      start_date = 1990, 
                      end_date = 2022,
                      return_wide=FALSE) %>% 
  filter(!is.na(value)) %>% 
  #drop unwanted variables
  select(-c(unit, obs_status, footnote, last_updated)) %>% 
  rename(year = date,
         co2percap = value)


# Download data for GDP per capita  https://data.worldbank.org/indicator/NY.GDP.PCAP.PP.KD
gdp_percap <- wb_data(country = "countries_only", 
                      indicator = "NY.GDP.PCAP.PP.KD", 
                      start_date = 1990, 
                      end_date = 2022,
                      return_wide=FALSE) %>% 
  filter(!is.na(value)) %>% 
  #drop unwanted variables
  select(-c(unit, obs_status, footnote, last_updated)) %>% 
  rename(year = date,
         GDPpercap = value)
```

Specific questions:

1.  How would you turn `energy` to long, tidy format?
2.  You may need to join these data frames
    -   Use `left_join` from `dplyr` to [join the tables](http://r4ds.had.co.nz/relational-data.html)
    -   To complete the merge, you need a unique *key* to match observations between the data frames. Country names may not be consistent among the three dataframes, so please use the 3-digit ISO code for each country
    -   An aside: There is a great package called [`countrycode`](https://github.com/vincentarelbundock/countrycode) that helps solve the problem of inconsistent country names (Is it UK? United Kingdom? Great Britain?). `countrycode()` takes as an input a country's name in a specific format and outputs it using whatever format you specify.
3.  Write a function that takes as input any country's name and returns all three graphs. You can use the `patchwork` package to arrange the three graphs as shown below

![](images/electricity-co2-gdp.png)

# Deliverables

There is a lot of explanatory text, comments, etc. You do not need these, so delete them and produce a stand-alone document that you could share with someone. Knit the edited and completed R Markdown (qmd) file as a Word or HTML document (use the "Knit" button at the top of the script editor window) and upload it to Canvas. You must be comitting and pushing your changes to your own Github repo as you go along.

# Details

-   Who did you collaborate with: TYPE NAMES HERE
-   Approximately how much time did you spend on this problem set: ANSWER HERE
-   What, if anything, gave you the most trouble: ANSWER HERE

**Please seek out help when you need it,** and remember the [15-minute rule](https://dsb2023.netlify.app/syllabus/#the-15-minute-rule){target="_blank"}. You know enough R (and have enough examples of code from class and your readings) to be able to do this. If you get stuck, ask for help from others, post a question on Slack-- and remember that I am here to help too!

> As a true test to yourself, do you understand the code you submitted and are you able to explain it to someone else?

# Rubric

13/13: Problem set is 100% completed. Every question was attempted and answered, and most answers are correct. Code is well-documented (both self-documented and with additional comments as necessary). Used tidyverse, instead of base R. Graphs and tables are properly labelled. Analysis is clear and easy to follow, either because graphs are labeled clearly or you've written additional text to describe how you interpret the output. Multiple Github commits. Work is exceptional. I will not assign these often.

8/13: Problem set is 60--80% complete and most answers are correct. This is the expected level of performance. Solid effort. Hits all the elements. No clear mistakes. Easy to follow (both the code and the output). A few Github commits.

5/13: Problem set is less than 60% complete and/or most answers are incorrect. This indicates that you need to improve next time. I will hopefully not assign these often. Displays minimal effort. Doesn't complete all components. Code is poorly written and not documented. Uses the same type of plot for each graph, or doesn't use plots appropriate for the variables being analyzed. No Github commits.
